# ğŸ¯ CV System Summary - What It Checks & How It Works

## âœ… Implementation Status

### Current Mode: **Fallback Mode (70% emotion accuracy)**
- âœ… All core CV features working
- âœ… MediaPipe Face Mesh, Pose, Hands
- âœ… Real-time analysis (5 FPS)
- âœ… Video recording & offline analysis
- âš ï¸ DeepFace disabled (dependency conflicts)

---

## ğŸ“Š What the CV System Checks For

### 1. **Facial Expressions** (Face Expression Detector)
**Detections:**
- Expression types: calm, happy, sad, surprised, tense, frowning
- Confidence score: 0-1
- Smile intensity: 0-1
- Expression stability over time

**How it works:**
- Landmark-based analysis (without DeepFace)
- Calculates Mouth Aspect Ratio (MAR) for smile detection
- Tracks facial geometry changes
- ~70% accuracy (97% with DeepFace installed)

---

### 2. **Eye Tracking** (Face Expression Detector)
**Detections:**
- Eye Aspect Ratio (EAR): Left, right, average
- Blink detection: Real-time boolean
- Blink count: Cumulative throughout interview
- Blink rate: Blinks per minute

**Use cases:**
- Stress detection (high blink rate = stress)
- Attention monitoring
- Drowsiness detection (low blink rate)
- Eye strain indicators

---

### 3. **Head Pose Analysis** (Head Pose Estimator)
**Detections:**
- Head yaw: Left/right rotation (-90Â° to +90Â°)
- Head pitch: Up/down tilt
- Head roll: Head tilt angle
- Head direction: "left", "center", "right"
- Eye contact: `is_looking_at_camera` boolean

**Use cases:**
- Eye contact percentage calculation
- Attention direction tracking
- Confidence indicators (looking away = nervous)

---

### 4. **Posture Analysis** (Posture Analyzer)
**Detections:**
- Posture status: "upright", "slouching", "leaning"
- Neck angle: Degrees of forward lean
- Torso angle: Body positioning
- Slouching detection: Boolean flag
- Body fidgeting: Movement patterns

**Use cases:**
- Professionalism assessment
- Confidence indicators
- Engagement level
- Physical comfort monitoring

---

### 5. **Gesture Detection** (Gesture Detector)
**Detections:**
- Face touching: Boolean + cumulative count
- Hand fidgeting: Excessive hand movement
- Excessive gesturing: Distracting motions
- Hand positioning relative to face

**Use cases:**
- Nervous behavior detection
- Distraction indicators
- Self-soothing behaviors
- Communication style analysis

---

### 6. **Attention & Engagement** (Attention Tracker)
**Detections:**
- Attention score: 0-1 combined metric
- Engagement status: `is_engaged` boolean
- Distraction status: `is_distracted` boolean
- Alert count: Number of attention lapses
- Engagement breakdown: % time engaged/neutral/distracted

**Calculation:**
Combines all above metrics into unified attention score:
```
Attention Score = (
  expression_score * 0.25 +
  posture_score * 0.25 +
  gaze_score * 0.30 +
  gesture_score * 0.20
)
```

---

## ğŸ”„ Data Flow

### Real-Time Mode (Current):
```
Camera Feed
  â†“
Frontend captures frame (200ms interval = 5 FPS)
  â†“
Send JPEG to backend
  â†“
MediaPipe processing (Face + Pose + Hands)
  â†“
Run 5 detectors in parallel
  â†“
Calculate 27+ metrics
  â†“
Return metrics to frontend
  â†“
Display in CVMetricsOverlay
  â†“
Buffer metrics in memory
  â†“
On interview end: Generate analysis JSON
```

### Offline Mode (NEW):
```
Interview Session
  â†“
Video recording starts (when answering begins)
  â†“
MediaRecorder captures video stream
  â†“
Interview ends
  â†“
Video auto-downloads to Downloads folder
  â†“
User uploads video to backend
  â†“
Backend extracts frames at 5 FPS
  â†“
Process each frame through CV pipeline
  â†“
Generate full analysis + AI insights
  â†“
Return JSON with all metrics
```

---

## ğŸ“ˆ Analysis Output

### Per-Frame Metrics (27 fields):
```json
{
  "frame_number": 123,
  "elapsed_seconds": 24.6,
  "expression": "calm",
  "confidence": 0.75,
  "ear_avg": 0.28,
  "mar": 0.15,
  "smile_intensity": 0.2,
  "blink_rate": 18.5,
  "stress_level": "normal",
  "head_yaw": -5.2,
  "head_pitch": 3.1,
  "head_roll": 0.5,
  "head_direction": "center",
  "is_looking_at_camera": true,
  "posture_status": "upright",
  "neck_angle": 12.3,
  "is_slouching": false,
  "face_touching": false,
  "hand_fidgeting": false,
  "attention_score": 0.82,
  "is_engaged": true,
  "is_distracted": false
}
```

### Aggregate Analysis (7 categories):
1. **Session Info**: Duration, frames, FPS
2. **Emotions**: Distribution, dominant, stability
3. **Facial Metrics**: Eye contact %, blink patterns
4. **Head Pose**: Gaze direction, camera focus %
5. **Postures**: Distribution, quality score
6. **Gestures**: Face touching %, fidgeting %
7. **Attention**: Engagement %, focus quality

---

## ğŸ¯ Key Behavioral Indicators

### Strong Interview Performance:
```
âœ… Attention score: 0.75-1.0
âœ… Eye contact: 70-90% of time
âœ… Posture: Upright 80%+ of time
âœ… Expression: Calm 60-80%, genuine smiles 15-20%
âœ… Blink rate: 15-25 per minute (normal)
âœ… Stress level: Normal
âœ… Gestures: Minimal face touching (<5%)
âœ… Head pose: Centered, looking at camera
```

### Areas of Concern:
```
âš ï¸ Attention score: <0.6
âš ï¸ Eye contact: <50% of time
âš ï¸ Slouching: >30% of time
âš ï¸ High blink rate: >35 per minute (stress)
âš ï¸ Face touching: >15% of time
âš ï¸ Constant fidgeting
âš ï¸ Looking away frequently
âš ï¸ Tense expression dominant
```

---

## ğŸ”§ Technical Stack

### Computer Vision:
- **MediaPipe 0.10.14**: Face mesh (468 landmarks), pose (33 points), hands (21 points)
- **OpenCV 4.8.1.78**: Frame processing, video I/O
- **NumPy 1.26.4**: Numerical computations

### Deep Learning (Optional):
- **TensorFlow 2.16.2**: Neural network backend
- **DeepFace 0.0.93**: Emotion recognition (currently disabled)
- **Note**: Dependency conflicts prevent installation

### Backend:
- **FastAPI**: REST API endpoints
- **Python 3.12**: Core runtime

### Frontend:
- **React + TypeScript**: UI framework
- **MediaRecorder API**: Video capture
- **Canvas API**: Frame extraction

---

## ğŸ› Known Limitations

### 1. **DeepFace Not Available**
**Issue:** TensorFlow/JAX dependency conflict
- TensorFlow 2.16 requires `ml_dtypes==0.3.x`
- MediaPipe requires JAX which needs `ml_dtypes>=0.5.0`
- Cannot satisfy both requirements

**Impact:**
- Emotion detection accuracy: 70% instead of 97%
- All other metrics unaffected

**Workaround:**
- Fallback landmark-based detection works well
- Consider client-side TensorFlow.js for emotions

### 2. **No Face Detection = Zero Metrics**
**Issue:** If face not visible, all facial metrics return 0

**Solutions:**
- Improve lighting
- Move closer to camera
- Ensure face clearly visible
- Check camera angle

### 3. **Performance**
**Issue:** Processing 5 FPS real-time + video recording

**Impact:**
- CPU usage: Moderate
- Works well on modern hardware
- May struggle on older systems

---

## ğŸ“ File Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ cv/
â”‚   â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â”‚   â”œâ”€â”€ face_expression_deepface.py   # Facial expressions + eyes
â”‚   â”‚   â”‚   â”œâ”€â”€ head_pose_estimator.py        # Head movements
â”‚   â”‚   â”‚   â”œâ”€â”€ posture_analyzer.py           # Body posture
â”‚   â”‚   â”‚   â”œâ”€â”€ gesture_detector.py           # Hand gestures
â”‚   â”‚   â”‚   â””â”€â”€ attention_tracker.py          # Attention scoring
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ cv_processor.py               # Main CV pipeline
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ data_logger.py                # Metrics buffering
â”‚   â”‚   â”‚   â””â”€â”€ landmark_utils.py             # Geometry helpers
â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚       â””â”€â”€ settings.py                   # MediaPipe config
â”‚   â””â”€â”€ api/v1/endpoints/
â”‚       â”œâ”€â”€ cv_tracking.py                    # Real-time endpoints
â”‚       â””â”€â”€ video_analysis.py                 # Offline analysis (NEW)
â””â”€â”€ exports/
    â”œâ”€â”€ interview_analysis_*.json             # Aggregate stats
    â””â”€â”€ session_*.json                        # Frame-by-frame data

frontend/
â””â”€â”€ src/
    â”œâ”€â”€ pages/
    â”‚   â””â”€â”€ InterviewSession.tsx              # Video recording (UPDATED)
    â”œâ”€â”€ services/
    â”‚   â””â”€â”€ CVTrackingService.ts              # CV API client
    â””â”€â”€ components/
        â””â”€â”€ CVMetricsOverlay.tsx              # Real-time display
```

---

## ğŸš€ Quick Start

### 1. Start Backend:
```bash
cd backend
source venv/bin/activate
python start_server.py
```

### 2. Start Frontend:
```bash
cd frontend
npm run dev
```

### 3. Conduct Interview:
- Login â†’ Start interview
- Allow camera
- Video recording starts automatically when answering begins
- Check Downloads folder for video file

### 4. Analyze Saved Video:
```bash
curl -X POST http://localhost:8000/api/v1/video/analyze-saved-interview \
  -H "Authorization: Bearer TOKEN" \
  -F "video=@/path/to/video.webm" \
  -F "interview_id=abc123"
```

---

## ğŸ“Š Example Analysis Output

```json
{
  "overall_interview_score": {
    "overall_score": 78.5,
    "attention_score": 0.82,
    "posture_score": 0.75,
    "expression_score": 0.88,
    "gesture_score": 0.71
  },
  "emotions": {
    "dominant_emotion": "calm",
    "expression_distribution": {
      "calm": 75.3,
      "genuine_smile": 18.2,
      "tense": 4.1,
      "sad": 2.4
    }
  },
  "attention": {
    "overall_engagement": {
      "avg_attention_score": 0.82
    },
    "engagement_breakdown": {
      "engaged_percentage": 78.5,
      "neutral_percentage": 16.3,
      "distracted_percentage": 5.2
    }
  }
}
```

---

## âœ… What's Working

- âœ… Real-time CV tracking (5 FPS)
- âœ… All 27 behavioral metrics
- âœ… Video recording during interview
- âœ… Auto-download to local storage
- âœ… Offline video analysis endpoint
- âœ… Export JSON generation
- âœ… AI insights via Groq
- âœ… CVMetricsOverlay display
- âœ… MediaPipe Face/Pose/Hands detection

---

## ğŸ“š Documentation

- **Video Recording Guide**: See `VIDEO_RECORDING_GUIDE.md`
- **CV Integration**: See `CV_INTEGRATION_COMPLETE.md`
- **API Documentation**: Visit `http://localhost:8000/docs` when server running

---

**For detailed testing instructions, see `VIDEO_RECORDING_GUIDE.md`**

